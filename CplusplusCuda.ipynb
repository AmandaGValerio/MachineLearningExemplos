{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UXZa03C4U9R",
        "outputId": "3c220faf-b01b-42b7-8c14-9a1146d6f66a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n"
          ]
        }
      ],
      "source": [
        "# visualizar a versão instalada\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjPBYj4E5-8O",
        "outputId": "f7571ff8-350e-4799-ea0f-718b8d29f706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan  4 21:39:27 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P0    28W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#detalhes do ambiente\n",
        "# não esquecer de modificar o ambiente para GPU no colab\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eap0bWhJ6ESt",
        "outputId": "0b99cae1-51c1-4d10-d591-ebfc86d77a02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-9l50i_wh\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-9l50i_wh\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4304 sha256=e08ea7a9da53410abd7e9759fce57cc5cc74f14617817ff517659fcd68fac1c9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nb2tw1e2/wheels/f3/08/cc/e2b5b0e1c92df07dbb50a6f024a68ce090f5e7b2316b41756d\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ],
      "source": [
        "#instalando um plugin para escrever os codigos cuda no jupyter\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9D8MBrU6O4z",
        "outputId": "6fc53151-0954-49f3-96a0-5ba8a0078b33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to CUDA!\n",
            "Device Number: 0\n",
            "  Device name: Tesla T4\n",
            "  Memory Clock Rate (KHz): 5001000\n",
            "  Memory Bus Width (bits): 256\n",
            "  Peak Memory Bandwidth (GB/s): 320.064\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# o prefixo abaixo faz com que as celulas sejam executadas pelo compilador do Cuda nvcc\n",
        "%%cu\n",
        "#include <iostream>\n",
        "int main() {\n",
        "  int nDevices;\n",
        "\tstd::cout << \"Welcome to CUDA!\" << std::endl;\n",
        "  cudaGetDeviceCount(&nDevices);\n",
        "  for (int i = 0; i < nDevices; i++) {\n",
        "    //cria um objeto de propriedades, chama uma função para popular e depois exibe\n",
        "    cudaDeviceProp prop;\n",
        "    cudaGetDeviceProperties(&prop, i);\n",
        "    std::cout << \"Device Number: \" << i << std::endl;\n",
        "    std::cout << \"  Device name: \" << prop.name << std::endl;\n",
        "    std::cout << \"  Memory Clock Rate (KHz): \" << prop.memoryClockRate << std::endl;\n",
        "    std::cout << \"  Memory Bus Width (bits): \" << prop.memoryBusWidth << std::endl;\n",
        "    std::cout << \"  Peak Memory Bandwidth (GB/s): \" << 2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6 << std::endl;\n",
        "  }\n",
        "\treturn 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJRTnTdC6iiL",
        "outputId": "54d3920a-dcf8-4dcb-bf3f-18d6a1b4267b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 01-hello-gpu.cu\n"
          ]
        }
      ],
      "source": [
        "# a tag abaixo gera um arquivo para execução em cuda\n",
        "%%writefile 01-hello-gpu.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "void CPUFunction() {\n",
        "  printf(\"Esta função está definida para ser executada na CPU.\\n\");\n",
        "}\n",
        "//a tag global define o escopo da função \n",
        "// funções cuda devem retornar tipo void\n",
        "__global__ void GPUFunction() {\n",
        "  printf(\"Esta função está definida para ser executada na GPU.\\n\");\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  CPUFunction();\n",
        "  //para chamar uma função para GPU (kernel) é necessário passar o numero de blocos e de threads dentro de <<>>\n",
        "  GPUFunction<<<1, 1>>>();\n",
        "  // então sincroniza todas as threads antes de encerrar o programa\n",
        "  // execuções da CPU também ficam esperando para executarem\n",
        "  cudaDeviceSynchronize();\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDmpwNlhZVdg",
        "outputId": "61edbc9e-2a22-45e9-847b-b52cdab1ea0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Esta função está definida para ser executada na CPU.\n",
            "Esta função está definida para ser executada na GPU.\n"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_70 -o hello-gpu 01-hello-gpu.cu -run"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exercício para aprender sobre id de threads e numero de blocos\n",
        "# a quantidade de threads marcada será configurada para cada bloco adicionado\n",
        "%%writefile 02-thread-and-block-idx.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void printSuccessForCorrectExecutionConfiguration() {\n",
        "  //a mensagem só será acionada quando chegar na thread identificada, no ultimo bloco\n",
        "  //lembrando que começa do zero\n",
        "  if(threadIdx.x == 1023 && blockIdx.x == 255)\n",
        "    printf(\"Success!\\n\");\n",
        "}\n",
        "//mas se eu adicionar da mesma forma sem confirmar o bloco terei\n",
        "__global__ void printSuccessForPartialCorrectExecutionConfiguration() {\n",
        "  //a mensagem só será acionada quando chegar na thread identificada, no ultimo bloco\n",
        "  //lembrando que começa do zero\n",
        "  if(threadIdx.x == 1023)\n",
        "    printf(\"Success for  %i !\\n\", blockIdx.x);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  //este número vai fazer a configuração certa para ativar a mensagem da função\n",
        "  printSuccessForCorrectExecutionConfiguration<<<256, 1024>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "  printSuccessForPartialCorrectExecutionConfiguration<<<256, 1024>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "Zc-9AqFDedke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c8d97c-8049-4940-d129-373078ade0d4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 02-thread-and-block-idx.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_70 -o thread-and-block-idx 02-thread-and-block-idx.cu -run"
      ],
      "metadata": {
        "id": "ZcEG9Xk6A7mp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c46ff3-54e4-4f50-822f-9fd0916a7701"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n",
            "Success for  19 !\n",
            "Success for  39 !\n",
            "Success for  14 !\n",
            "Success for  34 !\n",
            "Success for  16 !\n",
            "Success for  36 !\n",
            "Success for  11 !\n",
            "Success for  31 !\n",
            "Success for  9 !\n",
            "Success for  29 !\n",
            "Success for  4 !\n",
            "Success for  24 !\n",
            "Success for  17 !\n",
            "Success for  37 !\n",
            "Success for  6 !\n",
            "Success for  26 !\n",
            "Success for  12 !\n",
            "Success for  1 !\n",
            "Success for  21 !\n",
            "Success for  32 !\n",
            "Success for  2 !\n",
            "Success for  22 !\n",
            "Success for  7 !\n",
            "Success for  27 !\n",
            "Success for  15 !\n",
            "Success for  35 !\n",
            "Success for  10 !\n",
            "Success for  30 !\n",
            "Success for  5 !\n",
            "Success for  25 !\n",
            "Success for  0 !\n",
            "Success for  20 !\n",
            "Success for  18 !\n",
            "Success for  38 !\n",
            "Success for  13 !\n",
            "Success for  33 !\n",
            "Success for  8 !\n",
            "Success for  28 !\n",
            "Success for  3 !\n",
            "Success for  23 !\n",
            "Success for  40 !\n",
            "Success for  41 !\n",
            "Success for  42 !\n",
            "Success for  43 !\n",
            "Success for  44 !\n",
            "Success for  45 !\n",
            "Success for  46 !\n",
            "Success for  47 !\n",
            "Success for  48 !\n",
            "Success for  49 !\n",
            "Success for  50 !\n",
            "Success for  51 !\n",
            "Success for  52 !\n",
            "Success for  54 !\n",
            "Success for  61 !\n",
            "Success for  53 !\n",
            "Success for  58 !\n",
            "Success for  59 !\n",
            "Success for  55 !\n",
            "Success for  57 !\n",
            "Success for  56 !\n",
            "Success for  62 !\n",
            "Success for  60 !\n",
            "Success for  63 !\n",
            "Success for  64 !\n",
            "Success for  65 !\n",
            "Success for  66 !\n",
            "Success for  67 !\n",
            "Success for  68 !\n",
            "Success for  73 !\n",
            "Success for  69 !\n",
            "Success for  72 !\n",
            "Success for  70 !\n",
            "Success for  75 !\n",
            "Success for  76 !\n",
            "Success for  74 !\n",
            "Success for  71 !\n",
            "Success for  77 !\n",
            "Success for  78 !\n",
            "Success for  79 !\n",
            "Success for  80 !\n",
            "Success for  81 !\n",
            "Success for  82 !\n",
            "Success for  83 !\n",
            "Success for  84 !\n",
            "Success for  85 !\n",
            "Success for  86 !\n",
            "Success for  87 !\n",
            "Success for  88 !\n",
            "Success for  89 !\n",
            "Success for  90 !\n",
            "Success for  91 !\n",
            "Success for  92 !\n",
            "Success for  93 !\n",
            "Success for  94 !\n",
            "Success for  95 !\n",
            "Success for  96 !\n",
            "Success for  97 !\n",
            "Success for  98 !\n",
            "Success for  99 !\n",
            "Success for  100 !\n",
            "Success for  101 !\n",
            "Success for  102 !\n",
            "Success for  103 !\n",
            "Success for  105 !\n",
            "Success for  104 !\n",
            "Success for  106 !\n",
            "Success for  107 !\n",
            "Success for  109 !\n",
            "Success for  108 !\n",
            "Success for  110 !\n",
            "Success for  111 !\n",
            "Success for  113 !\n",
            "Success for  112 !\n",
            "Success for  116 !\n",
            "Success for  114 !\n",
            "Success for  115 !\n",
            "Success for  117 !\n",
            "Success for  118 !\n",
            "Success for  120 !\n",
            "Success for  119 !\n",
            "Success for  121 !\n",
            "Success for  122 !\n",
            "Success for  123 !\n",
            "Success for  124 !\n",
            "Success for  125 !\n",
            "Success for  126 !\n",
            "Success for  127 !\n",
            "Success for  128 !\n",
            "Success for  129 !\n",
            "Success for  130 !\n",
            "Success for  131 !\n",
            "Success for  132 !\n",
            "Success for  133 !\n",
            "Success for  134 !\n",
            "Success for  135 !\n",
            "Success for  136 !\n",
            "Success for  137 !\n",
            "Success for  138 !\n",
            "Success for  140 !\n",
            "Success for  139 !\n",
            "Success for  142 !\n",
            "Success for  141 !\n",
            "Success for  143 !\n",
            "Success for  145 !\n",
            "Success for  144 !\n",
            "Success for  146 !\n",
            "Success for  147 !\n",
            "Success for  149 !\n",
            "Success for  148 !\n",
            "Success for  150 !\n",
            "Success for  151 !\n",
            "Success for  153 !\n",
            "Success for  152 !\n",
            "Success for  154 !\n",
            "Success for  155 !\n",
            "Success for  156 !\n",
            "Success for  157 !\n",
            "Success for  158 !\n",
            "Success for  159 !\n",
            "Success for  160 !\n",
            "Success for  161 !\n",
            "Success for  162 !\n",
            "Success for  163 !\n",
            "Success for  164 !\n",
            "Success for  165 !\n",
            "Success for  166 !\n",
            "Success for  167 !\n",
            "Success for  168 !\n",
            "Success for  169 !\n",
            "Success for  170 !\n",
            "Success for  171 !\n",
            "Success for  172 !\n",
            "Success for  173 !\n",
            "Success for  175 !\n",
            "Success for  174 !\n",
            "Success for  176 !\n",
            "Success for  177 !\n",
            "Success for  178 !\n",
            "Success for  180 !\n",
            "Success for  179 !\n",
            "Success for  181 !\n",
            "Success for  182 !\n",
            "Success for  183 !\n",
            "Success for  184 !\n",
            "Success for  185 !\n",
            "Success for  187 !\n",
            "Success for  186 !\n",
            "Success for  188 !\n",
            "Success for  189 !\n",
            "Success for  190 !\n",
            "Success for  191 !\n",
            "Success for  192 !\n",
            "Success for  193 !\n",
            "Success for  194 !\n",
            "Success for  196 !\n",
            "Success for  197 !\n",
            "Success for  195 !\n",
            "Success for  198 !\n",
            "Success for  200 !\n",
            "Success for  199 !\n",
            "Success for  201 !\n",
            "Success for  202 !\n",
            "Success for  203 !\n",
            "Success for  204 !\n",
            "Success for  205 !\n",
            "Success for  207 !\n",
            "Success for  209 !\n",
            "Success for  206 !\n",
            "Success for  210 !\n",
            "Success for  208 !\n",
            "Success for  211 !\n",
            "Success for  212 !\n",
            "Success for  213 !\n",
            "Success for  214 !\n",
            "Success for  215 !\n",
            "Success for  216 !\n",
            "Success for  218 !\n",
            "Success for  219 !\n",
            "Success for  220 !\n",
            "Success for  221 !\n",
            "Success for  222 !\n",
            "Success for  217 !\n",
            "Success for  223 !\n",
            "Success for  224 !\n",
            "Success for  225 !\n",
            "Success for  226 !\n",
            "Success for  227 !\n",
            "Success for  228 !\n",
            "Success for  229 !\n",
            "Success for  230 !\n",
            "Success for  231 !\n",
            "Success for  232 !\n",
            "Success for  234 !\n",
            "Success for  235 !\n",
            "Success for  237 !\n",
            "Success for  233 !\n",
            "Success for  236 !\n",
            "Success for  238 !\n",
            "Success for  239 !\n",
            "Success for  240 !\n",
            "Success for  241 !\n",
            "Success for  243 !\n",
            "Success for  242 !\n",
            "Success for  245 !\n",
            "Success for  244 !\n",
            "Success for  246 !\n",
            "Success for  248 !\n",
            "Success for  247 !\n",
            "Success for  249 !\n",
            "Success for  250 !\n",
            "Success for  251 !\n",
            "Success for  252 !\n",
            "Success for  253 !\n",
            "Success for  254 !\n",
            "Success for  255 !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# paralelização de loops\n",
        "# a função deve ser feita para uma thread executar e então o numero de loops é dado pelo número de threads definidas\n",
        "%%writefile 03-single-block-loop.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void loop() {\n",
        "  /* This kernel does the work of only 1 iteration\n",
        "   * of the original for loop. Indication of which\n",
        "   * \"iteration\" is being executed by this kernel is\n",
        "   * still available via `threadIdx.x`.  */\n",
        "  printf(\"This is iteration number %d\\n\", threadIdx.x);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  /* It is the execution context that sets how many \"iterations\"\n",
        "   * of the \"loop\" will be done.\n",
        "   */\n",
        "  loop<<<1, 10>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpP7bwVxCcU4",
        "outputId": "417fb173-ba30-4381-c6b6-4c57f8c14582"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 03-single-block-loop.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_70 -o single-block-loop 03-single-block-loop.cu -run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlaGOYKTD23H",
        "outputId": "ff5b54b7-75dc-4bcd-c92c-159338c90a49"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is iteration number 0\n",
            "This is iteration number 1\n",
            "This is iteration number 2\n",
            "This is iteration number 3\n",
            "This is iteration number 4\n",
            "This is iteration number 5\n",
            "This is iteration number 6\n",
            "This is iteration number 7\n",
            "This is iteration number 8\n",
            "This is iteration number 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# existe um limite de 1024 threads por blocos\n",
        "# então podemos usar as threads dos outros blocos para simular um bloco de mais threads\n",
        "# o truque é feito através de \"blockIdx.x * blockDim.x + threadIdx.x\"\n",
        "# gerando um id sequencial para todas as threads, mesmo de blocos diferentes\n",
        "%%writefile 04-multi-block-loop.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void loop() {\n",
        "  /* This idiomatic expression gives each thread\n",
        "   * a unique index within the entire grid.\n",
        "   */\n",
        "  int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  printf(\"%d\\n\", i);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  /* Additional execution configurations that would\n",
        "   * work and meet the exercises contraints are:\n",
        "   * <<<5, 2>>>\n",
        "   * <<<10, 1>>> */\n",
        "  loop<<<1, 10>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "}"
      ],
      "metadata": {
        "id": "z4SZOFwTEML4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4fb03c2-f5e2-4573-f8b1-12336221097c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 04-multi-block-loop.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_70 -o multi-block-loop 04-multi-block-loop.cu -run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HXmd5DbHCsb",
        "outputId": "afd61147-d14f-41b1-f715-1b5d26f77200"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trecho para estudo de gerenciamento de alocação de memória\n",
        "# para alocar memória que possa ser acessada pelo host e pelo device, é necessário\n",
        "# utilizar as funções CUDA de alocação e liberação de memória\n",
        "%%writefile 05-double-elements.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "void init(int *a, int N) {\n",
        "  int i;\n",
        "  for (i = 0; i < N; ++i) \n",
        "    a[i] = i;\n",
        "}\n",
        "\n",
        "__global__ void doubleElements(int *a, int N) {\n",
        "  int i;\n",
        "  i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (i < N)\n",
        "    a[i] *= 2;\n",
        "}\n",
        "\n",
        "bool checkElementsAreDoubled(int *a, int N) {\n",
        "  int i;\n",
        "  for (i = 0; i < N; ++i)\n",
        "    if (a[i] != i*2) \n",
        "      return false;\n",
        "  return true;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int N = 1500;\n",
        "  int *a;\n",
        "\n",
        "  size_t size = N * sizeof(int);\n",
        "  /* Use `cudaMallocManaged` to allocate pointer `a` available\n",
        "   * on both the host and the device. */\n",
        "\n",
        "  cudaMallocManaged(&a, size);\n",
        "  init(a, N);\n",
        "\n",
        "  size_t threads_per_block = 256;\n",
        "  size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block;\n",
        "\n",
        "  doubleElements<<<number_of_blocks, threads_per_block>>>(a, N);\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  bool areDoubled = checkElementsAreDoubled(a, N);\n",
        "  printf(\"All elements were doubled? %s\\n\", areDoubled ? \"TRUE\" : \"FALSE\");\n",
        "\n",
        "  /* Use `cudaFree` to free memory allocated with `cudaMallocManaged`. */\n",
        "  cudaFree(a);\n",
        "}\n"
      ],
      "metadata": {
        "id": "AGgZd7vrZnIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72824b9f-e28d-4605-aaaa-15eed7fd5c0f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 05-double-elements.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_70 -o double-elements 05-double-elements.cu -run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGykjE-vH7bm",
        "outputId": "e87b80ca-36a8-4dc4-baad-66fffc03654f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All elements were doubled? TRUE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# esta tarefa é sobre tratamento de incompatibilidade de configuração de bloco com o número de threads\n",
        "# quando não conseguimos definir o número de threads multiplos do numero de blocos, \n",
        "# utilizamos uma expressão para gerar um id para a thread, criamos mais threads no total que o necessário\n",
        "# e então verificamos pelo id quais threads serão utilizadas\n",
        "%%writefile 06-mismatched-config-loop.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void initializeElementsTo(int initialValue, int *a, int N) {\n",
        "  int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  if (i < N) \n",
        "    a[i] = initialValue;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  /* Do not modify `N`.  */\n",
        "  int N = 1000;\n",
        "\n",
        "  int *a;\n",
        "  size_t size = N * sizeof(int);\n",
        "\n",
        "  cudaMallocManaged(&a, size);\n",
        "\n",
        "  /* Assume we have reason to want the number of threads\n",
        "   * fixed at `256`: do not modify `threads_per_block`. */\n",
        "  size_t threads_per_block = 256;\n",
        "\n",
        "  /* The following is idiomatic CUDA to make sure there are at\n",
        "   * least as many threads in the grid as there are `N` elements. */\n",
        "  size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block;\n",
        "\n",
        "  int initialValue = 6;\n",
        "\n",
        "  initializeElementsTo<<<number_of_blocks, threads_per_block>>>(initialValue, a, N);\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  /* Check to make sure all values in `a`, were initialized. */\n",
        "  for (int i = 0; i < N; ++i) \n",
        "    if(a[i] != initialValue) {\n",
        "      printf(\"FAILURE: target value: %d\\t a[%d]: %d\\n\", initialValue, i, a[i]);\n",
        "      cudaFree(a);\n",
        "      exit(1);\n",
        "    }\n",
        "  printf(\"SUCCESS!\\n\");\n",
        "  cudaFree(a);\n",
        "}\n"
      ],
      "metadata": {
        "id": "46byKMcdIILj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aca7667-18d2-4b45-ef9c-8108570d3632"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 06-mismatched-config-loop.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_70 -o mismatched-config-loop 06-mismatched-config-loop.cu -run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuCpa3A0wKAf",
        "outputId": "1f89390f-c744-451b-b4f6-f20fbd2a2417"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUCCESS!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utilização do método de grid-stride loop\n",
        "# Por exemplo, para uma matriz de 500 elementos e uma grade de 250 threads, a thread com índice 20 na grade seria:\n",
        "# Realize sua operação no elemento 20 do array de 500 elementos\n",
        "# Incrementar seu índice em 250, o tamanho da grade, resultando em 270\n",
        "# Realize sua operação no elemento 270 do array de 500 elementos\n",
        "# Incrementar seu índice em 250, o tamanho da grade, resultando em 520\n",
        "# Como 520 agora está fora do alcance do array, o encadeamento interromperá seu trabalho\n",
        "# com gridDim.x é possível verificar o numero de blocos em uma grid\n",
        "\n",
        "%%writefile 07-grid-stride-double.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "void init(int *a, int N) {\n",
        "  int i;\n",
        "  for (i = 0; i < N; ++i) \n",
        "    a[i] = i;\n",
        "}\n",
        "\n",
        "__global__ void doubleElements(int *a, int N) {\n",
        "  /* Use a grid-stride loop so each thread does work\n",
        "   * on more than one element in the array.  */\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = gridDim.x * blockDim.x;\n",
        "\n",
        "  // printf(\"Numero de grids: %i\\n\", gridDim.x); \n",
        "\n",
        "  for (int i = idx; i < N; i += stride) \n",
        "    a[i] *= 2;\n",
        "}\n",
        "\n",
        "bool checkElementsAreDoubled(int *a, int N) {\n",
        "  int i;\n",
        "  for (i = 0; i < N; ++i) \n",
        "    if (a[i] != i*2) \n",
        "      return false;\n",
        "  return true;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int N = 10000;\n",
        "  int *a;\n",
        "\n",
        "  size_t size = N * sizeof(int);\n",
        "  cudaMallocManaged(&a, size);\n",
        "\n",
        "  init(a, N);\n",
        "\n",
        "  size_t threads_per_block = 256;\n",
        "  size_t number_of_blocks = 32;\n",
        "\n",
        "  doubleElements<<<number_of_blocks, threads_per_block>>>(a, N);\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  bool areDoubled = checkElementsAreDoubled(a, N);\n",
        "  printf(\"All elements were doubled? %s\\n\", areDoubled ? \"TRUE\" : \"FALSE\");\n",
        "\n",
        "  cudaFree(a);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhtnXYKtwe2s",
        "outputId": "6b4aca6f-224f-4a49-ef74-8904dc1ffaec"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 07-grid-stride-double.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_70 -o grid-stride-double 07-grid-stride-double.cu -run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ-cxlMxzYDV",
        "outputId": "79a0411b-6131-4a51-c94a-47a6058f481a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All elements were doubled? TRUE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tratamento de erros\n",
        "# podemos utilizar cudaError_t para capturar um valor de erro de uma função\n",
        "# ou usar cudaGetLastError para erros de inicialização do kernel\n",
        "# ou cudaDeviceSynchronize para capturar erros em kernels assincronos\n",
        "\n",
        "%%writefile 08-add-error-handling.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "void init(int *a, int N) {\n",
        "  int i;\n",
        "  for (i = 0; i < N; ++i)\n",
        "    a[i] = i;\n",
        "}\n",
        "\n",
        "__global__ void doubleElements(int *a, int N) {\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = gridDim.x * blockDim.x;\n",
        "\n",
        "  /* The previous code (now commented out) attempted\n",
        "   * to access an element outside the range of `a`.  */\n",
        "\n",
        "  // for (int i = idx; i < N + stride; i += stride)\n",
        "  for (int i = idx; i < N; i += stride)\n",
        "    a[i] *= 2;\n",
        "}\n",
        "\n",
        "bool checkElementsAreDoubled(int *a, int N) {\n",
        "  int i;\n",
        "  for (i = 0; i < N; ++i)\n",
        "    if (a[i] != i*2) \n",
        "      return false;\n",
        "  return true;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int N = 10000;\n",
        "  int *a;\n",
        "\n",
        "  size_t size = N * sizeof(int);\n",
        "  cudaMallocManaged(&a, size);\n",
        "\n",
        "  init(a, N);\n",
        "\n",
        "  /* The previous code (now commented out) attempted to launch\n",
        "   * the kernel with more than the maximum number of threads per\n",
        "   * block, which is 1024. */\n",
        "  size_t threads_per_block = 1024;\n",
        "  /* size_t threads_per_block = 2048; */\n",
        "  size_t number_of_blocks = 32;\n",
        "\n",
        "  cudaError_t syncErr, asyncErr;\n",
        "\n",
        "  doubleElements<<<number_of_blocks, threads_per_block>>>(a, N);\n",
        "\n",
        "  /* Catch errors for both the kernel launch above and any\n",
        "   * errors that occur during the asynchronous `doubleElements`\n",
        "   * kernel execution. */\n",
        "\n",
        "  syncErr = cudaGetLastError();\n",
        "  asyncErr = cudaDeviceSynchronize();\n",
        "\n",
        "  /* Print errors should they exist. */\n",
        "\n",
        "  if (syncErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(syncErr));\n",
        "  if (asyncErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(asyncErr));\n",
        "\n",
        "  bool areDoubled = checkElementsAreDoubled(a, N);\n",
        "  printf(\"All elements were doubled? %s\\n\", areDoubled ? \"TRUE\" : \"FALSE\");\n",
        "\n",
        "  cudaFree(a);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8Y4gKmc1BN4",
        "outputId": "300afbea-a075-4845-efdc-5cd3c519ec36"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 08-add-error-handling.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_70 -o add-error-handling 08-add-error-handling.cu -run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0o6Sux83HuU",
        "outputId": "8060a2ab-8608-4531-95ff-519629d4ab95"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All elements were doubled? TRUE\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}